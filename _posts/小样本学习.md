

# 小样本学习

[TOC]

## 1 背景

美团的各个业务中，有着丰富的NLP场景，而这些场景中模型的构建，需要许多的标注资源。但是，标注资源往往是有限的，人工标注的成本较高，很难获取大量的标注数据。小样本学习致力于在数据资源稀少的情况下训练出比较好的模型，主要有以下两个评价标准：

- 提升算法效果：在标注资源一定的情况下，使用小样本学习能够尽可能多的提升相应的指标；
- 节省标注数据：在算法效果希望达到一定水平的情况下，希望尽可能的减少标注数据。

在NLP领域少样本场景下，主要存在以下三种场景，小样本学习针对这些问题采取不同措施：

1. 样本空间稀缺 (图1左)：样本数量较少时，分布稀缺，数据增强旨在更好利用样本/embedding之间的关系，提高模型泛化性能。
2. 样本分布在局部空间 (图1中)：对于某个领域往往只有少量标注数据，而有大量的未标注数据。根据对未标注数据的使用方式不同我们将其划分为两种，第一种是半监督学习，是在模型fintune过程中同时学习标注样本和未标注样本，利用了模型对未标注数据的预测一致性；第二种是集成学习+自训练，强调的是融合多个模型对未标注数据的预测结果作为伪标注数据加入训练。
3. 不同领域之间的样本分布差异 (图1右)：在某个领域充分学习到标注信息之后，因样本空间有差异，无法直接用到其他领域，迁移学习旨在学习到一个领域的充分知识后，能够快速学习到其他领域知识。

![image-20220914153525833](/Users/zhz/Library/Application Support/typora-user-images/image-20220914153525833.png)             

除了上面提到的三种场景， 还有一种是如何在有限的标注成本中选择更有针对性的样本进行人工标注（主动学习）。因此我们将小样本学习划分为下面几种：

- 数据增强：数据增强可以分为样本增强和Embedding增强。样本增强早先在计算机视觉中对图像进行数据增强，图像的一些简单操作，如将图像旋转或将其转换为灰度，并不会改变其语义，语义不变变换的存在使增强成为计算机视觉研究中的一个重要工具。在NLP领域中的样本增强也试图不改变句子主旨来扩充文本数据，主要方法有简单文本替换、预训练语言模型生成相似句子等，对得到的增强数据可以使用课程学习的方式由简到难地进行学习。Embedding增强在模型的embedding层进行操作，可以通过对embedding加扰动/插值等，提升模型的鲁棒性。
- 半监督学习：监督学习往往需要大量的标注数据，而标注数据的成本比较高，因此如何利用大量的无标注数据来提高监督学习的效果，具有十分重要的意义。近年来，半监督深度学习取得了长足的进展，特别是在计算机视觉领域；相关的研究主要着力于如何针对未标注数据构建无监督信号，与监督学习联合建模，目前的主要方法都是基于无标注数据的一致性正则构建损失函数。
- 集成学习+自训练：机器学习的有监督学习目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况中有时只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。单纯使用多个模型在预测时做集成会增加线上负担，因此我们利用多个模型对大量无标注数据进行预测，选取组合置信度较高的数据合并到训练集进行训练，最后将多个模型的优势集成到一个统一的模型上。这部分也属于半监督学习的一种，但是与上面提到的半监督学习的主要区别是后者提到的方法都是强调在fintune阶段利用模型对未标注数据的预测一致性，而这部分强调融合多个模型的预测结果，故单独将这部分单独列出。
- Few-shot Learning/领域迁移：人类在具备一定的知识储备后可以快速学习新的知识，研究人员希望机器学习模型也能具备这种能力，一个模型已经从一定类别的大量数据学习许多信息后，对于新的类别可以依据新的少量的标记样本得到新的分类器，该分类器可以在新的类别中识别出该类样本。
- 主动学习：主动学习是一个机器学习和人工参与迭代的过程，通过机器学习的方法筛选出合适的候选集给人工标注。大致思路是：通过机器学习的方法获取到那些比较“难”分类的样本数据，让人工再次确认和审核，然后再将人工标注得到的数据再次使用有监督学习模型进行训练，逐步提升模型效果。

​    ![1028628807](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/1028628807.png)

## 2 方法综述

预训练语言模型BERT在NLP许多任务中取得非常好的结果，BERT是基于Transformer的深度双向语言表征模型，利用Transformer结构构造了一个多层双向的Encoder网络。我们采取BERT作为Baseline模型，对预训练好的模型在特定任务样本上进行Finetune。

### 2.1 数据增强

数据增强可以分为数据扩充和Embedding增强，数据扩充在保持语义不变的情况下，变换文本的表达形式，如回译、同义词替换、随机删除等等；模型增强主要有Mixup和对抗训练，Mixup在NLP领域有一系列的变体，包括 SeqMix、Manifold Mixup等。数据增强提高了模型的鲁棒性，使得模型更关注文本的语义信息，并对文本的局部噪声不再敏感。在少样本场景，文本增强技术可以有效提高模型的鲁棒性，提高其泛化能力。

#### 2.1.1 样本增强

##### 2.1.1.1 简单数据增强EDA

EDA[1]通过知识库来替换句子中的某些词/短语，主要包含以下四种操作：

- 同义词替换（Synonyms Replacement, SR）：不考虑停用词，在句子中随机抽取n个词，然后从同义词词典中随机抽取同义词，并进行替换。

- 随机插入(Random Insertion, RI)：随机的找出句中某个不属于停用词集的词，并求出其随机的同义词，将该同义词插入句子的一个随机位置。重复n次；

- 随机交换(Random Swap, RS)：随机的选择句中两个单词并交换它们的位置。重复n次；

- 随机删除(Random Deletion, RD)：以概率p随机删除句子中每个单词。

  EDA增强示例

  原句：生活里的惬意，无需等到春暖花开

  同义词替换：生活里的惬意，并不需要等到春暖花开

  随机插入：生活里面的惬意，无需舒适等到春暖花开

  随机交换：生活里的惬意，等到无需春暖花开

  随机删除：生活里的，等到春暖花开

##### **2.1.1.2 回译(**Back Translation)

回译是NLP在机器翻译中经常使用的一个数据增强的方法，其本质就是快速产生一些翻译结果达到增加数据的目的，回译将原有数据翻译为其他语言再翻译回原语言，由于语言逻辑顺序等的不同，回译的方法也往往能够得到和原数据差别较大的新数据。

回译示例

原句：生活里的惬意，无需等到春暖花开

中—>英—>中：生活的舒适，无需等到春天开花

中—>日—>中：生活的舒适，无需等到春天的花朵

中—>德—>中：生活的舒适，无需等到春天开花

中—>法—>中：生活的舒适，无需等待春天的花朵

##### 2.1.1.3 预训练语言模型

- **基于上下文信息的文本增强 [2]** 

利用训练好的语言模型，随机mask文中的一个词或字，再将文本输入语言模型，选择语言模型所预测的 top k 个词去替换原文中被去掉的词，以形成 k 条新的文本。

- **基于语言生成模型的文本增强 LAMBDA [3]**

LAMBADA 基于预训练语言模型GPT，使模型能够捕获语言的结构，从而能产生连贯的句子。在不同任务的少量数据集上对模型进行微调，并使用微调后的模型生成新的句子。

#### 2.1.2 增强样本使用

上面几种方式生成了一批数据增强文本，增强后的文本数量多、带噪音；原始标注数据数据量少、不含噪音。 NAACL2021[4]的一篇工作提出了一种课程学习的方式更好地学习这两种样本。

![946620399](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/946620399.png)

- **Standard Data Augmentation**

  - 直接将原始数据和增强后的数据融合起来进行训练

- **Curriculum Data Augmentation**

  课程学习先学习简单的标注数据，具备一定知识之后再学习带噪音的增强数据，有以下两种使用方式

  - **Two stage：** 先在原始数据上进行训练，等到开发集收敛之后再在原始数据和增强数据上一起训练
  - **Gradual：**先在原始数据上进行训练，然后逐步以线性方式添加增强数据，通过控制扰动变量*τ*, 即一开始*τ*=0，然后每次线性增加0.1，直到增加到0.5

#### 2.1.3 Embedding增强

##### 2.1.3.1 Mixup

自然语言具备系统组合性(Systematic Compositionality)，能够理解通过适当重组的句子。考虑以下样例，左边两个为原始文本，通过对其中的某些词进行替换/重组可以生成右边可理解的样本。

​                     

![image-20220914153621151](/Users/zhz/Library/Application Support/typora-user-images/image-20220914153621151.png)

Mixup[6][7]提出了一种更通用的，基于向量增强的模型，从训练数据中任抽样两个样本，构造混合样本和混合标签，作为新的增广数据。其中(*x**i*,*y**i*)和(*x**j*,*y**j*)为原始样本，(*x*^,*y*^)为重组生成的新样本。当*λ*的取值限制在{0, 1}，就会产生图4的组合。

*x*^*y*^=*λ**x**i*+(1−*λ*)*x**j*,=*λ**y**i*+(1−*λ*)*y**j*,

实验中*λ*的取值满足Beta分布

*λ*∼Beta(*α*,*α*)

直观来看，它要求当模型的输入为另外两个输入的线性组合时，其输出也是这两个数据单独输入模型后所得输出的线性组合，这其实就是要求模型近似为一个线性系统，防止模型过拟合, Mixup 变换本身可以视为一种正则化技术。 

下面介绍几种Mixup的变体, SeqMix和Manifold Mixup。               

###### a. SeqMix

SeqMix[8]对Mixup生成的句子的困惑度打分，用discriminator判断生成的句子困惑度，只选择困惑度较低的句子，减少噪音句子的影响。同时对于更关注于词的任务（如命名实体识别）替换sub-sequence而不是整个句子。

![img](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/946620039.png)

图5. SeqMix的几种变体

- 图5(a)最原始的Mixup, 对整个序列做Mixup，生成新序列
- 图5(b)子序列Mixup: 对合法的子序列做Mixup，并分别代替原来的子序列，生成两个新序列
- 图5(c)标签限制的子序列Mixup: 只对同类别标签的子序列做Mixup，分别代替原来的子序列，生成两个新序列

###### b. Manifold Mixup 

流形混合算法Manifold Mixup[9]将上述的Mixup操作泛化到特征上；特征具有更高阶的语义信息，在其维度上插值可能会产生更有意义的样本。在类似于BERT的模型中，随机选择层数*k*，对该层的特征表示进行Mixup，具体操作如下：

- 随机选取网络第*k*层（包括输入层）
- 传两批数据给网络，前向传播直到第*k*层，得到隐藏特征表示 (*g* *k*(*x*),*y*)和(*g* *k* (*x*′),*y*′)
- 对其进行Mixup生成新样本(*g*~*k*,*y*~)=(Mixup*λ*(*g* *k*(*x*),*g* *k*(*x*′)),(Mixup*λ*(*y*,*y*′))
- 继续前向传播得到输出
- 计算损失值和梯度

##### 2.1.3.2 对抗训练 

对抗训练(*Adversarial training, AT*) [10]通过在输入样本上增加微小的扰动来显著提高模型loss，对抗训练就是训练一个能有效识别原始样本和对抗样本的模型。如果模型在对抗噪声下，依然能够保持光滑，那么整个网络就能够表现出很好的一致性。当将对抗训练应用到分类器时，对抗训练对应的损失函数为(在原有损失函数的基础上又增加了一项):

−*l**o**g*(*p*(*y*∣*x*+*r**a**d**v*);*θ*);where *r**a**d**v*=arg*r*,∣∣*r*∣∣≤*ϵ*minlog*p*(*y*∣*x*+*r*;*θ*~)

其中*x*为输入序列，*θ*为模型参数，*r*为输入上的扰动，*θ*~表示将模型的当前参数设置为常数，即表明在构建对抗样本时，反向传播算法不会对当前模型参数进行更新。其中*r**a**d**v*的计算：

*r**a**d**v*=*ϵ*∣∣*g*∣∣2*g* where *g*=∇*x*log*p*(*y*∣*x*;*θ*~)

##### 2.1.3.3 对比学习 R-Drop

Dropout是深度学习模型常用的正则化手段， Regularized Dropout (R-Drop)[11]对同一个句子做两次dropout，并且强制由dropout生成的不同子模型的输出概率保持一致。 具体来说，对于每个训练样本，R-Drop 最小化了由不同dropout生成的子模型的输出概率之间的 KL 散度。

![969209208](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/969209208.png)

图6. R-Drop模型图

图6中左边图表示了每个输入样本(*x*,*y*)都会经过模型两次，得到两个概率分布，右图展示了由于dropout本身的随机性，对同一个样本重复两次就可以得到两个子模型。

给定训练数据*D*={(*x**i*,*y**i*)}*i*=1*n*，模型为*P**w*(*y*∣*x*)， 训练数据的交叉熵损失为

*L**C**E*=*n*1*i*=1∑*n*−log*P**w*(*y**i*∣*x**i*)

通过不同的dropout可以得到两个子模型*P*1*w*(*y*∣*x*)和*P*2*w*(*y*∣*x*)，R-Drop最小化两个子模型输出概率的双向KL散度

*L**K**L**i*=21(*D**K**L*(*P*1*w*(*y**i*∣*x**i*)∣∣*P*2*w*(*y**i*∣*x**i*))+*D**K**L*(*P*2*w*(*y**i*∣*x**i*)∣∣*P*1*w*(*y**i*∣*x**i*)))

对于训练数据(*x**i*,*y**i*)，最终的损失函数为

*L**i*=*L**C**E**i*+*α**L**K**L**i*=−log*P*1*w*(*y**i*∣*x**i*)−log*P*2*w*(*y**i*∣*x**i*)+2*α*(*D**K**L*(*P*1*w*(*y**i*∣*x**i*)∣∣*P*2*w*(*y**i*∣*x**i*))+*D**K**L*(*P*2*w*(*y**i*∣*x**i*)∣∣*P*1*w*(*y**i*∣*x**i*)))

#### 2.1.4 P-tuning

链接：https://kexue.fm/archives/8295, https://github.com/THUDM/P-tuning

在小样本学习中，引入模版，对模版进行微调：

**第一种，标注数据比较少。**这种情况下，我们固定整个模型的权重，只优化[unused1]～[unused6]这几个token的Embedding，换句话说，其实我们就是要学6个新的Embedding，使得它起到了模版的作用。这样一来，因为模型权重几乎都被固定住了，训练起来很快，而且因为要学习的参数很少，因此哪怕标注样本很少，也能把模版学出来，不容易过拟合。

**第二种，标注数据很充足。**这时候如果还按照第一种的方案来，就会出现欠拟合的情况，因为只有6个token的可优化参数实在是太少了。因此，我们可以放开所有权重微调，原论文在SuperGLUE上的实验就是这样做的。读者可能会想：这样跟直接加个全连接微调有什么区别？原论文的结果是这样做效果更好，可能还是因为跟预训练任务更一致了吧。

![P-tuning直接使用[unused*]的token来构建模版，不关心模版的自然语言性](https://kexue.fm/usr/uploads/2021/04/2868831073.png)

### 2.2 半监督学习

在实际业务中，有大量的未标注数据，对未标注数据进行标注需要大量的标注人力支持。半监督学习的核心目标是，希望通过标注的少量有标签数据，结合大量的无标签数据，训练出具备强泛化能力的模型，从而解决实际中的问题：

- 输入数据：大量的同领域未标注数据和少量的有标注数据。
- 主要基于以下理论基础：

1. **自洽正则化(Consistency Regularization)**

   对未标记的数据进行增广，产生的增广数据输入模型，其预测值应该与原数据的预测值一致，即保持自洽。

2. **最小化熵(Entropy Minimization)**

   基于一个规则：分类器的分类边界不应该穿过边际分布的高密度区域。具体做法为强迫分类器对未标记数据作出低熵预测。

3. **传统正则化(Traditional Regularization)**

   为了提高模型泛化能力，防止过拟合的方法，如L2正则化等。

- 如何构造半监督学习的自洽正则化项是很多半监督模型研究的重点
  - Temporal Ensemble、Mean Teacher、MixTemporal 这几种方法的思想都是利用了对历史模型的集成，利用历史模型预测结果/模型参数构造一致性正则；
  - VAT则是利用了给embedding加上扰动构造正则项；
  - MixMatch、MixText、UDA则是利用了样本增强的结果，MixMatch利用了图像的旋转、缩放等，MixText和UDA依赖于通过回译生成的样本之间的一致性，模型结果很大一部分取决于样本增强的质量。

#### 2.2.1 Temporal Ensembling

Temporal ensembling[12]采用时序融合模型，将历史预测结果的指数滑动平均(EMA)作为未标注数据的伪标签，从而和当前的预测结果构造一致性正则项。

- 有标注数据计算交叉熵损失
- 未标注数据采用历史多次预测结果的指数滑动平均(EMA)作为重构目标计算MSE损失，避免采用模型一次的预测作为重构项会产生较大误差，有利于平滑单次预测中的噪声
- 有标注数据的交叉熵损失和未标注数据的均方误差加权和作为最终损失函数

![946760647](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/946760647.png)

  图7. Temporal ensembling模型结构

其中*x**i*中包含有标注数据和未标注数据文本，*y**i*表示标注数据标签，*z**i*表示当前模型对*x**i*的预测结果， *z*~*i*表示对未标注数据多个epoch预测结果的滑动平均值作为其概率分布，*w*(*t*)表示未标注数据MSE所占权重，模型刚开始训练时*w*(*t*)−>0，此时模型倾向于学习标注数据标签，随着迭代次数增加，*w*(*t*)慢慢增加，此时模型学习大量未标注数据。

#### **2.2.2 Mean Teacher**

Mean Teacher[13]思想上与Temporal ensembling模型基本一致，Temporal ensembling需要矩阵保存历史预测概率，Mean Teacher模型则是对模型参数做指数滑动平均EDA作为teacher模型，原模型作为student模型。用teacher模型和student模型的预测结构构造一致性正则。

- 在 Temporal ensembling 中，无标签数据的目标标签来自模型前几个epoch预测结果的加权平均。而在 Mean Teacher 中，无标签数据的目标标签来自 teacher 模型的预测结果；

- 由于是通过模型参数的平均来实现标签预测，因此在每个step都可以把无标签中的信息更新到模型中，而不必像 Temporal ensembling 模型需要等到一个epoch结束再更新。

  ![946698615](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/946698615.png)

  图8. Mean Teacher 模型结构

#### 2.2.3 VAT

虚拟对抗网络(*Virtual adversarial training, VAT* )[14] 与 Temporal ensembling模型的不同之处在于，后者采用数据增强、dropout 来对无标签数据施加噪声，而前者施加的则是模型变化最陡峭方向上的噪声，即所谓的对抗噪声。如果模型在对抗噪声下，依然能够保持光滑，那么整个网络就能够表现出很好的一致性。虚拟对抗训练通过对模型添加正则使得一个样本的输出分布与加扰动之后的输出分布相同来将对抗训练扩展到半监督领域，整个过程和对抗训练 (section 2.1.6) 的是一致的。

- 首先对未标注数据抽取随机标准正态扰动*d*∼N(0,1)加到embedding上，并用KL散度计算梯度
- 然后用得到的梯度计算对抗扰动，并进行对抗训练
- 标注数据的交叉熵损失和未标注数据对抗损失作为最终损失函数

#### 2.2.4 MixMatch

MixMatch[15]是集大成者，将数据增强、Mixup、Sharpening等方法融合起来，起重要作用的两个模块就是Mixup和Sharpening。

![946753113](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/946753113.png)

图9. MixMatch无标注数据标签构造

- 在图像领域未标注数据的*k*条增强数据来自图像的旋转、缩放等
- 输入标注数据*X*=(*x**b*,*p**b*), 未标注数据*U*=(*u**b*), 利用图9生成未标注数据的标签*q**b*，*q**b*即是模型对未标注数据和它的*k*条增强数据预测结果的平均值
- 将*X*=(*x**b*,*p**b*)和*U*=(*u**b*,*q**b*)混合做Mixup操作
- 计算标注数据的交叉熵损失和未标注数据的一致性正则损失项

#### 2.2.5 MixText

MixText [16]整个思路和MixMatch是一致的。首先利用back translation对未标注数据做数据增强，利用模型对增强后的数据和原数据的预测结果的加权和作为未标注数据的标签， 再对有标注数据和未标注数据同时做Mixup。

- 无标注数据通过back translation做数据增强
- 利用模型对增强后的数据和原数据的预测结果的加权和作为无标注数据的标签
- 同时对有标注数据和无标注数据做Mixup
- 学习有标注数据的交叉熵损失和无标注数据的一致性正则

![946659819](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/946659819.png)

图10. MixText模型结构图

其中*x**l*是标注样本，*x**u*是未标注样本，*x**a*是未标注样本的回译增强样本。与Manifold Mixup (section 2.1.5.b) 相似，同样是对模型第*m*层的隐藏表示做Mixup，可以挖掘到句子之间的隐式关系。对有标签和无标签数据同时进行插值，可以在学习有标签句子的同时利用无标签句子的信息。

#### **2.2.6 MixTemporal**

借鉴MixMatch和MixText思想，但对无标注数据的预测不是通过EDA或者回译这种数据增强，而是利用Temporal ensemling的思想，对多个epoch历史预测结果的指数滑动平均(EMA)作为未标注数据的伪标签，不需要额外的同义词典和翻译工具等，实现较为方便。

最终损失函数计算两部分组成：Mixup的之后的交叉熵loss + 无标注数据一致性正则loss

![image-20220914154138246](/Users/zhz/Library/Application Support/typora-user-images/image-20220914154138246.png)

图11. MixTemporal模型结构图

其中*x**i*是标注样本，*y**i*表示标注样本标签，*z**i*表示当前模型对*x**i*的预测结果， *z*~*i*表示对未标注数据多个epoch预测结果的滑动平均值作为其概率分布，*w*(*t*)表示未标注数据MSE所占权重。

#### 2.2.7 UDA

UDA[17]来自Google，同样也是采用一致性正则。对于图像UDA针对图像采取一种高质量的数据增强方法RandAugment，对于文本UDA使用的是回译和非核心词替换，用 TF-IDF值来衡量一个词对于一段文本的重要性，来判断该词是否要被替换，再联合回译做替换。

![946659820](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/946659820.png)

图12. UDA模型结构图

训练技巧：

- Confidence-based masking：通过阈值筛选一致性正则项
- Sharpening Predictions：让预测的概率分布更加极端化
- Domain-relevance Data Filtering： 去掉领域不相关的数据，同样根据置信度来选择

### 2.3 集成学习+自训练

在有监督学习算法中，目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。如果待组合的各个模型之间差异性(diversity)比较显著，那么Ensemble之后通常会有一个较好的结果。自训练使用少量的标记数据和大量的未标记数据对模型进行联合训练，首先使用经过训练的分类器来预测所有未标记数据的标签，然后选择置信度较高的标签作为伪标注数据，将伪标记数据与人工标记的训练数据联合起来重新训练分类器。

- 集成学习：训练多个不同的模型，如BERT模型，Mixup模型，半监督学习模型
  - 分别用每个模型预测数据池U(Unlabeled data)的标签概率分布
  - 计算标签概率分布的加权和，得到数据池U的概率分布soft prediction
- 自训练：训练一个模型用于组合其他各个模型
  - Student模型学习数据池U中高置信度样本的soft prediction
  - Student模型作为最后的强学习器

![image-20220914154254864](/Users/zhz/Library/Application Support/typora-user-images/image-20220914154254864.png)

​        图13. 集成学习+自训练结构图

### 2.4 领域迁移

领域迁移主要解决少样本，多类别的问题

- MAML[18]：2017年发表被广泛引用的方法，但是训练过程较为复杂，后面被Meta-Baseline方法在多个领域超越了。
- Meta-Baseline[19]：2020年发表的新基线方法，大幅超越前者，且方法简单，因此称为New Baseline。Meta-Baseline方法，通过在所有base classes上预先训练分类器，并在基于最近质心的少样本(few-shot)分类算法上进行元学习。
  - Classifier-Baseline: 在所有的base classes上使用交叉熵损失预训练分类器，然后移除最后一层FC，得到feature encoder *f**θ*;
  - Meta-Baseline:
    - pre-training stage：训练Classifier-Baseline模型；
    - meta-learning stage： 给定一个few-shot task（N-way k-shot），计算每个类别的特征平均值*w**c*=∣*S**c*∣1*x*∈*s**c*∑*f**θ*(*x*)
    - 针对于每个query-set中的样本用余弦相似度计算与每个类别的相似度，减少类内方差

![946698616](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/946698616.png)

图14.Meta-Baseline模型结构图

### 2.5 主动学习

Active Learning（主动学习）通过一定的算法查询最有用的未标记样本，并交由专家进行标记，然后用查询到的样本训练分类模型来提高模型的精确度。 模型抛出的未标注数据为“hard sample”，对于“hard sample”的不同定义可以衍生出一大堆的方法，如可以是ambiguous sample，即模型最难区分的样本；可以是对模型提升（改变）最大的样本，如梯度提升最大；可以是方差减小等等，相比与有监督学习，主动学习通过让模型更多的关注或学习“hard sample”，以期在较少的训练样本下获得较好的模型。

- 查询策略是主动学习的核心之处，不确定性采样的查询是最常用的，不确定性采样的关键是描述样本的不确定性，通常有如下思路
  - 置信度最低（Least Confident），如对于三分类的情况下，对两个样本预测的概率分别是(0.8, 0.1, 0.1)和(0.51, 0.31, 0.18)，在这种情况下第二个数据更“难”被区分，因此更有标注价值。
  - 边缘采样（Margin Sampling），选择那些更容易被判定成两类的样本数据，即这些数据被判定成两类的概率相差不大，边缘采样选择模型预测最大和第二大的概率差值最小的样本。
  - 熵方法（Entropy），用熵来衡量一个系统的不确定性，熵越大表示系统的不确定性越大，熵越小表示系统的不确定性越小，可以选择那些熵比较大的样本数据作为待标注数据。
- 迭代思路

输入：初始少量标注数据L0、未标注数据池U、深度学习模型M

1. 标注数据集L<-L0
2. 用L训练模型M，并对未标注数据池U进行预测
3. 用对应的查询策略选择U中需要标注的样本进行标注，并将其加入到标注数据集L中
4. 重复2-3过程，直到达到精度或者或者标注预算

![857728096](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/857728096.png)图15.主动学习迭代过程

## 3 应用实践

图16展示了基于BERT[20]的模型微调，包括句间关系任务和单句分类任务。

1. 单句分类任务: BERT模型预训练中的NSP任务使得BERT中的“[CLS]”位置的输出包含了整个句子对（句子）的信息，我们利用其在有标注的数据上微调模型，给出预测结果。常见单句分类任务如文本分类、情感分类等。
2. 句间关系任务: 对两个句子用“[SEP]”进行拼接，用“[CLS]”位置的输出判断两个句子之间的关系。常见句间关系任务如自然语言推理、语义相似度判断等。

![864236280](/Users/zhz/Downloads/小样本学习及其在美团场景中的应用/864236280.png)

(a) 句间关系任务                                            (b) 单句分类任务

图16. 基于BERT的微调模型

### 3.1 实验结果

在上面我们提到了主要有以下两个评价标准，提升算法效果和节省标注数据，我们分别在美团点评业务和通用Benchmark数据集上进行了微调实验。

**1. 小样本学习，提升算法效果**



表1和表2列出了四种embedding增强，两种半监督学习，以及集成学习+自训练模型的结果。我们也尝试过上文提到的其他方法，如UDA/MixText，这些方法依赖于外部翻译软件翻译的结果，效果不稳定。因此我们主要使用了以上模型，几种模型的比较如下：

- 数据增强：四种embedding增强结果中，对抗训练(AT)表现最为稳定，平均可提升模型1pp。
- 半监督学习：数据增强相对于半监督学习结果较为稳定，但半监督学习可以带来的提升较为明显，可提升模型1.5-2pp，其中在AFQMC数据集上提升模型2-4pp。
- 集成学习+自训练：集成学习+自训练融合了单模型在未标注数据上的预测结果，基本可以达到或这接近最好的结果，平均可提升模型1.5-2pp。

**2. 主动学习，减少训练样本**

主动学习(active learning)选择置信度较低的样本用作下一批标注，可以减少重复/相似样本标注，降低标注成本。在新项目标注数据从无到有的时候，可以使用active learning选择启动样本。

- 在目前数据集下，平均500条active learning选择数据可以达到1000条随机样本结果，900条数据可以接近1500条随机样本结果。在新项目标注数据从无到有的时候，可以使用active learning选择启动样本。 

### 3.2 在美团点评业务中的应用

#### 3.2.1 医美题材分类

对美团和点评的笔记内容按题材分为8类：猎奇、探店、测评、真人案例、治疗过程、避坑、效果对比、科普。用户点击某一种题材时，返回对应的笔记内容，上线至美团和点评App医疗美容频道的百科页、方案页经验分享。该任务是典型的文本分类任务，小样本学习利用2,989条训练数据准确率达到了89.24%。

#### 3.2.2 攻略识别

从UGC和笔记中挖掘旅游攻略，提供旅游攻略的内容供给，应用于景点精搜下的攻略模块，召回内容为描述旅游攻略的笔记。该任务为二分类任务，小样本学习利用384条训练数据准确率达到了87%。

#### 3.2.3 医美功效打标

对美团和点评的笔记内容按功效进行召回，功效的类型有：补水、美白、瘦脸、除皱等，上线至医美频道页。该任务是句间关系任务，全量笔记内容为104w，有110种功效类型需要打标，小样本学习仅用2909条训练数据准确率达到了91.88%。

#### 3.2.4 医美品牌打标

品牌上游企业有针对旗下产品进行品牌宣传和营销的诉求，而内容营销是当前主流、有效的营销方式之一。品牌打标就是为每种品牌如“伊肤泉”、“术唯可”召回详细介绍该品牌的笔记内容。先用品牌词进行匹配，再判断品牌词和匹配到的笔记内容的相关性，是详细描述该品牌，还是简单提及到品牌名字，上线至医美品牌馆。该任务是句间关系任务，有103种品牌，大类品牌15种每种标注64条数据，其余品牌每种标注5-8条数据， 小样本学习仅用1676条训练数据准确率达到了88.59%。

#### 3.2.5 **其他业务应用**

- 医美评价真实性：点评和美团医美评论中存在许多用户感知为假的，对体验有比较大的伤害，需要通过模型检测出来。通过Active Learning选取标注数据，数据增强、半监督学习和集成学习+自训练优化模型，最后业务方仅标注1757条数据，模型在检测感知虚假评价的准确率上就能达到95.88%，超出业务预期
- POI打标情感分析：该任务为句间关系任务，判断query和内容的情感导向(正面、负面、不确定和不相关)。已有模型在1W条数据上训练，通过小样本学习，在已有模型上提升模型精度0.63pp。
- 学城文本分类：该任务为文本分类任务，将文本分为17种类别。已有模型在700条数据上训练，通过小样本学习，在已有模型上提升模型精度2.5pp。

## 4 未来展望

1. **持续改进现有模型、探索更多模型**

   目前的实验结果还有很大的改进空间，需要不断探索，改进模型；同时探索更多的领域迁移模型，并应用到业务中，达成业务方可以用最少的数据训练最好的结果。

2. **在更多任务类型上进行实验**

   目前主要是单句分类、句间分类等任务类型上进行实验，需要进一步MRC模型、命名实体识别模型。

3. **深入探索领域迁移，训练通用模型**

   目前，我们对接的业务较多，因此也积累了不少领域的文本分类和句间关系数据集，希望能够训练一个通用模型用于该领域的各项任务，从而在一个新的业务中使用更少量的数据就能达到很好的业务效果。如可以通过Facebook的EFL[21]模型，将该领域下的文本分类任务和句间关系任务都重新表述为文本蕴含任务训练一个通用模型，可以直接在新的业务上进行迁移。

4. **建设小样本学习平台**

   目前正在将小样本学习能力集成到搜索与NLP部的BERT平台，开放给公司各业务方灵活使用。后续，在对小样本学习进行了更加深入的探索之后，我们会尝试建立单独的小样本学习平台，提供更多的低资源学习能力。

## 5 参考文献

【1】Wei J, Zou K. Eda: Easy data augmentation techniques for boosting performance on text classification tasks[J]. arXiv preprint arXiv:1901.11196, 2019.

【2】Kobayashi S. Contextual augmentation: Data augmentation by words with paradigmatic relations[J]. arXiv preprint arXiv:1805.06201, 2018.

【3】Anaby-Tavor A, Carmeli B, Goldbraich E, et al. Not Enough Data? Deep Learning to the Rescue![J]. arXiv preprint arXiv:1911.03118, 2019.

【4】Wei J, Huang C, Vosoughi S, et al. Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning[J]. arXiv preprint arXiv:2103.07552, 2021.

【5】Andreas J. Good-enough compositional data augmentation[J]. arXiv preprint arXiv:1904.09545, 2019.

【6】Zhang H, Cisse M, Dauphin Y N, et al. Mixup: Beyond empirical risk minimization[J]. arXiv preprint arXiv:1710.09412, 2017.

【7】Guo D, Kim Y, Rush A M. Sequence-level mixed sample data augmentation[J]. arXiv preprint arXiv:2011.09039, 2020.

【8】Zhang R, Yu Y, Zhang C. Seqmix: Augmenting active sequence labeling via sequence mixup[J]. arXiv preprint arXiv:2010.02322, 2020.

【9】Verma V, Lamb A, Beckham C, et al. Manifold mixup: Better representations by interpolating hidden states[C]//International Conference on Machine Learning. PMLR, 2019: 6438-6447.

【10】Miyato T, Dai A M, Goodfellow I. Adversarial training methods for semi-supervised text classification[J]. arXiv preprint arXiv:1605.07725, 2016.

【11】Liang X, Wu L, Li J, et al. R-Drop: Regularized Dropout for Neural Networks[J]. arXiv preprint arXiv:2106.14448, 2021.

【12】Laine S, Aila T. Temporal ensembling for semi-supervised learning[J]. arXiv preprint arXiv:1610.02242, 2016.

【13】Tarvainen A, Valpola H. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results[J]. arXiv preprint arXiv:1703.01780, 2017.

【14】Miyato T, Maeda S, Koyama M, et al. Virtual adversarial training: a regularization method for supervised and semi-supervised learning[J]. IEEE transactions on pattern analysis and machine intelligence, 2018, 41(8): 1979-1993.

【15】Berthelot D, Carlini N, Goodfellow I, et al. Mixmatch: A holistic approach to semi-supervised learning[J]. arXiv preprint arXiv:1905.02249, 2019.

【16】Chen J, Yang Z, Yang D. Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification[J]. arXiv preprint arXiv:2004.12239, 2020.

【17】Xie Q, Dai Z, Hovy E, et al. Unsupervised data augmentation for consistency training[J]. arXiv preprint arXiv:1904.12848, 2019.

【18】Finn C, Abbeel P, Levine S. Model-agnostic meta-learning for fast adaptation of deep networks[C]//International Conference on Machine Learning. PMLR, 2017: 1126-1135.

【19】Chen Y, Wang X, Liu Z, et al. A new meta-baseline for few-shot learning[J]. arXiv preprint arXiv:2003.04390, 2020.

【20】Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.

【21】Wang S, Fang H, Khabsa M, et al. Entailment as Few-Shot Learner[J]. arXiv preprint arXiv:2104.14690, 2021.