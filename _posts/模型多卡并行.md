[TOC]

## 模型部署

要将模型并行部署在多个 GPU 上，你可以使用 PyTorch 提供的 `torch.nn.DataParallel` 或 `torch.nn.parallel.DistributedDataParallel`。这里我将向你展示如何使用 `DistributedDataParallel`，因为它在许多场景下提供更好的性能和扩展性。

首先，确保你已经安装了 PyTorch 和 transformers。接下来，按照以下步骤操作：

1. 初始化环境变量和后端。

```python
import os
import torch
from torch import distributed

os.environ['MASTER_ADDR'] = 'localhost'
os.environ['MASTER_PORT'] = '12355'
distributed.init_process_group(backend='nccl', init_method='env://')
```

2. 设置 GPU 设备。

```python
local_rank = int(os.environ['LOCAL_RANK'])
torch.cuda.set_device(local_rank)
device = torch.device("cuda", local_rank)
```

3. 载入模型和分布式配置。

```python
from transformers import MegatronBertConfig, MegatronBertModel
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("IDEA-CCNL/Erlangshen-MegatronBert-1.3B")
config = MegatronBertConfig.from_pretrained("IDEA-CCNL/Erlangshen-MegatronBert-1.3B")
model = MegatronBertModel.from_pretrained("IDEA-CCNL/Erlangshen-MegatronBert-1.3B")
```

4. 将模型包装在 `DistributedDataParallel` 中。

```python
from torch.nn.parallel import DistributedDataParallel as DDP

model = model.to(device)
model = DDP(model, device_ids=[local_rank], output_device=local_rank)
```

5. 准备数据和训练。

准备好数据并使用 DataLoader 进行分批。在训练循环中，使用模型和优化器执行前向传播、反向传播和优化。注意，在使用 `DistributedDataParallel` 时，应使用 `DistributedSampler` 对数据进行采样。

6. 完成训练后，销毁进程组。

```python
distributed.destroy_process_group()
```

最后，请注意，你需要使用多 GPU 运行此脚本。例如，如果你使用的是 4 个 GPU，可以通过以下命令运行脚本：

```sh
bashCopy code
python -m torch.distributed.launch --nproc_per_node=4 your_script.py
```

在这个例子中，`your_script.py` 是包含上述代码的 Python 脚本。

## 数据读取

要在 PyTorch 的 DataLoader 中使用 `DistributedSampler`，你需要首先从 `torch.utils.data.distributed` 导入它。然后，你需要根据当前进程的总数和索引来设置 `DistributedSampler`。以下是一个示例：

1. 导入必要的库。

```
pythonCopy code
import torch
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler
```

1. 创建一个示例数据集（这里我们使用一个简单的数据集作为示例，你需要用你自己的数据集替换它）。

```
pythonCopy code
class ExampleDataset(Dataset):
    def __len__(self):
        return 1000

    def __getitem__(self, idx):
        return torch.tensor([idx], dtype=torch.float32)
```

2. 初始化分布式环境。

```
pythonCopy code
import os
from torch import distributed

os.environ['MASTER_ADDR'] = 'localhost'
os.environ['MASTER_PORT'] = '12355'
distributed.init_process_group(backend='nccl', init_method='env://')
```

3. 设置 GPU 设备。

```
pythonCopy code
local_rank = int(os.environ['LOCAL_RANK'])
torch.cuda.set_device(local_rank)
device = torch.device("cuda", local_rank)
```

4. 创建数据集和数据加载器。

```
pythonCopy code
dataset = ExampleDataset()
sampler = DistributedSampler(dataset, num_replicas=distributed.get_world_size(), rank=distributed.get_rank())

dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)
```

5. 然后，在训练循环中使用这个 DataLoader。

```
pythonCopy code
for epoch in range(num_epochs):
    sampler.set_epoch(epoch)  # 在每个 epoch 开始时更新采样器
    for batch in dataloader:
        # 执行训练步骤（前向传播、损失计算、反向传播、优化器更新等）
```

注意，在每个 epoch 开始时，你需要调用 `sampler.set_epoch(epoch)` 来确保在不同的 epoch 中洗牌数据。这对于在分布式训练中同步所有进程至关重要。

现在，你的 DataLoader 已经使用了 `DistributedSampler`，可以在多 GPU 上进行分布式训练。记得像之前的回答中所示，使用 `torch.distributed.launch` 命令运行脚本。



## 区别

`DataParallel` 和 `DistributedDataParallel` 都是 PyTorch 提供的并行训练方法，它们可以将模型部署在多个 GPU 上。然而，它们在实现和性能上有一些关键区别。

**DataParallel**

- 使用方法简单，只需将模型包装在 `torch.nn.DataParallel` 类中。
- 仅适用于单节点多 GPU 的情况。
- 在每个批次的前向和后向传播过程中，主 GPU（通常为 GPU 0）负责收集和分发数据以及梯度。因此，主 GPU 的显存负载较大，可能会成为性能瓶颈。
- 对于小批量训练或不太消耗资源的模型，`DataParallel` 可能足够了。但是，在许多情况下，`DistributedDataParallel` 提供了更好的性能和扩展性。

**DistributedDataParallel**

- 实现起来稍微复杂一点，需要进行分布式环境和后端的初始化，以及正确地设置 GPU 设备和本地排名。
- 适用于多节点多 GPU 的情况，更具扩展性。
- 使用分布式通信库（如 NCCL、Gloo 等）在 GPU 之间进行通信，实现更高效的数据和梯度交换。
- 在训练过程中，每个 GPU 都独立地执行前向和后向传播，并使用梯度累计和同步方法更新模型权重。这消除了单个 GPU 成为性能瓶颈的问题。
- 通常情况下，在大多数训练场景中，`DistributedDataParallel` 提供更好的性能。

总结一下，`DistributedDataParallel` 是一种更高效、更具扩展性的并行训练方法，适用于多节点多 GPU 的情况。尽管实现起来略微复杂，但它通常能带来更好的性能。而 `DataParallel` 使用起来更简单，但仅限于单节点多 GPU 的情况，并可能受到主 GPU 性能瓶颈的影响。如果可能的话，优先考虑使用 `DistributedDataParallel`。

## 不同服务器多卡部署

要在不同服务器（节点）上的多个 GPU 上进行分布式训练，你需要使用 `DistributedDataParallel`。以下是一个分布式训练的示例，涵盖了如何在多个节点上设置和运行训练。

1. 首先，创建一个名为 `train.py` 的脚本，并确保所有节点上都有相同的脚本和数据。`train.py` 应该包含以下代码：

```
pythonCopy code
import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# 初始化分布式环境
def init_distributed(backend='nccl'):
    dist.init_process_group(backend=backend)

# 设置 GPU 设备
def set_device():
    local_rank = int(os.environ['LOCAL_RANK'])
    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)
    return device, local_rank

# 在这里创建你的模型、损失函数、优化器和数据加载器
# ...

def main():
    # 初始化分布式环境
    init_distributed()

    # 设置 GPU 设备
    device, local_rank = set_device()

    # 在这里载入你的模型、损失函数、优化器和数据加载器
    # ...

    # 将模型包装在 DistributedDataParallel 中
    model = model.to(device)
    model = DDP(model, device_ids=[local_rank], output_device=local_rank)

    # 开始训练
    for epoch in range(num_epochs):
        # ...
        # 在这里编写你的训练循环

if __name__ == "__main__":
    main()
```

1. 安装 `torch.distributed.launch` 模块。如果你使用的是 PyTorch 1.8 以上版本，它已经预先包含在 PyTorch 包中。
2. 在训练脚本所在目录中创建一个名为 `hosts.txt` 的文件，其中包含所有节点的 IP 地址（每行一个）。例如：

```
Copy code
192.168.0.1
192.168.0.2
192.168.0.3
```

1. 确保所有节点上安装了相同版本的 PyTorch 和相关依赖库。
2. 在每个节点上运行以下命令以启动分布式训练。其中，`--nproc_per_node` 参数表示每个节点上的 GPU 数量。例如，如果每个节点上有 4 个 GPU，则命令如下：

```
bashCopy code
python -m torch.distributed.launch --nnodes=3 --node_rank=0 --master_addr="192.168.0.1" --master_port=12355 --nproc_per_node=4 train.py
```

请注意，`--node_rank` 参数需要在每个节点上分别设置。在第一个节点上，`--node_rank=0`；在第二个节点上，`--node_rank=1`；以此类推。

现在，你的分布式训练应该在多个节点的多个 GPU 上运行。所有节点上的 GPU 都将参与训练，并使用分布式通信库（如 NCCL）