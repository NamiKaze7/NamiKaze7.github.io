#  xgboost

blog：https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.3%20XGBoost/3.3%20XGBoost.md

https://zhuanlan.zhihu.com/p/126536571

### 总览

XGBoost（eXtreme Gradient Boosting）是一种集成学习方法，它基于Gradient Boosting（梯度提升）算法。XGBoost的目标是提高模型的准确性和执行效率。它常用于分类、回归和排序任务。

XGBoost的算法原理可以概括为以下几点：

1. 集成学习：XGBoost使用集成学习方法，它将多个弱学习器（通常是决策树）组合在一起，形成一个强学习器。每个弱学习器侧重于弥补前一个模型的不足，以提高整体模型的性能。
2. 梯度提升：XGBoost使用梯度提升算法，每次迭代时，都会计算损失函数的梯度（关于预测值的偏导数），并使用这个梯度信息来构建新的弱学习器。新学习器的目标是最小化整体损失函数。
3. 剪枝：与传统的梯度提升算法不同，XGBoost在构建决策树时会进行剪枝。这意味着它会移除不重要的分支，防止模型过拟合。剪枝策略有预剪枝和后剪枝两种。
4. 正则化：XGBoost引入了正则化项，用于控制模型的复杂度。正则化项包括了树的叶子节点数量和树的权重，有助于减小过拟合风险。
5. 并行计算：虽然梯度提升本身是一个顺序过程，但XGBoost使用了一些技巧（例如特征排序和分位数近似）来加速计算过程。这使得XGBoost在计算上更加高效。
6. 处理缺失值：XGBoost具有处理缺失值的能力。在构建决策树时，它会为缺失值寻找最佳的分裂方向，从而使得整体损失最小化。

总之，XGBoost是一个高性能的梯度提升算法，它通过集成多个弱学习器、剪枝策略、正则化、并行计算和处理缺失值等方法，提高了模型的准确性和执行效率。

### XGBoost树的定义

先来举个**例子**，我们要预测一家人对电子游戏的喜好程度，考虑到年轻和年老相比，年轻更可能喜欢电子游戏，以及男性和女性相比，男性更喜欢电子游戏，故先根据年龄大小区分小孩和大人，然后再通过性别区分开是男是女，逐一给各人在电子游戏喜好程度上打分，如下图所示。

![image-20230322153507921](/Users/zhz/Library/Application Support/typora-user-images/image-20230322153507921.png)

就这样，训练出了2棵树tree1和tree2，类似之前gbdt的原理，两棵树的结论累加起来便是最终的结论，所以小孩的预测分数就是两棵树中小孩所落到的结点的分数相加：2 + 0.9 = 2.9。爷爷的预测分数同理：-1 + （-0.9）= -1.9。具体如下图所示：

![image-20230322153528131](/Users/zhz/Library/Application Support/typora-user-images/image-20230322153528131.png)



## 模型原理 

### 优化目标

![image-20230310164841721](/Users/zhz/Library/Application Support/typora-user-images/image-20230310164841721.png)

![image-20230310173146531](/Users/zhz/Library/Application Support/typora-user-images/image-20230310173146531.png)

### 核心思想

1. **不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数f(x)，去拟合上次预测的残差。**
2. **当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数**
3. **最后只需要将每棵树对应的分数加起来就是该样本的预测值。**

### 权重怎么来

![image-20230310173735999](/Users/zhz/Library/Application Support/typora-user-images/image-20230310173735999.png)

### 如何选择下一棵树

那接下来，我们如何选择每一轮加入什么 f 呢？答案是非常直接的，选取一个 f 来使得我们的目标函数尽量最大地降低。这里 f 可以使用泰勒展开公式近似。

![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415343865867530120.png)

实质是把样本分配到叶子结点会对应一个obj，优化过程就是obj优化。也就是分裂节点到叶子不同的组合，不同的组合对应不同obj，所有的优化围绕这个思想展开。到目前为止我们讨论了目标函数中的第一个部分：训练误差。

XGBoost对树的复杂度包含了两个部分：

- 一个是树里面叶子节点的个数T
- 一个是树上叶子节点的得分w的L2模平方（对w进行L2正则化，相当于针对每个叶结点的得分增加L2平滑，目的是为了避免过拟合）

![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438674199471483.png)

我们再来看一下XGBoost的目标函数（损失函数揭示训练误差 + 正则化定义复杂度）：

![image-20230310171004873](/Users/zhz/Library/Application Support/typora-user-images/image-20230310171004873.png)

正则化公式也就是目标函数的后半部分，对于上式而言，𝑦_𝑖'是整个累加模型的输出，正则化项∑kΩ(ft)是则表示树的复杂度的函数，值越小复杂度越低，泛化能力越强。

### 树如何分裂

**枚举所有不同树结构的贪心法**

选择一个feature分裂，计算loss function最小值，然后再选一个feature分裂，又得到一个loss function最小值，你枚举完，找一个效果最好的，把树给分裂，就得到了小树苗。**每次在上一次的预测基础上取最优进一步分裂/建树**

#### 分裂停止

1. 当引入的分裂带来的增益小于设定阀值的时候，我们可以忽略掉这个分裂，所以并不是每一次分裂loss function整体都会增加的，有点预剪枝的意思，阈值参数为（即**正则项里叶子节点数T的系数**）；
2. 当树达到最大深度时则停止建立决策树，设置一个超参数**max_depth**，避免树太深导致学习局部样本，从而过拟合；
3. 样本权重和小于设定阈值时则停止建树。什么意思呢，即涉及到一个超参数-最小的样本权重和**min_child_weight**，和GBM的 min_child_leaf 参数类似，但不完全一样。大意就是一个叶子节点样本太少了，也终止同样是防止过拟合；

![image-20230310174557752](/Users/zhz/Library/Application Support/typora-user-images/image-20230310174557752.png)