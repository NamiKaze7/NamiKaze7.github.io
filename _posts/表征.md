

# 表征学习

[TOC]

## SimCSE

![preview](https://pic3.zhimg.com/v2-b4af1a43af3e853e1c59c691ba5b4232_r.jpg)

（1）将一只猫的图X，数据增强的方式生成另一张猫的图片作为正例X+，构建正例样本对，选择一只狗作为负例X-。

（2）将这个正负例样本组（X,X+,X-）同时输入到一个模型中进行特征抽取。

（3）优化对比损失，将X和X+的在特征空间里面的距离拉近，同时将X和X-在特征空间中的距离拉远。

```python
def compute_loss(y_pred,lamda=0.05):
    row = torch.arange(0,y_pred.shape[0],3,device='cuda') # [0,3]
    col = torch.arange(y_pred.shape[0], device='cuda') # [0,1,2,3,4,5]
   #这里[(0,1,2),(3,4,5)]代表二组样本，
   #其中0,1是相似句子，0,2是不相似的句子
   #其中3,4是相似句子，3,5是不相似的句子
    col = torch.where(col % 3 != 0)[0].cuda() # [1,2,4,5]
    y_true = torch.arange(0,len(col),2,device='cuda') # 生成真实的label  = [0,2]
   #计算各句子之间的相似度，形成下方similarities 矩阵，其中xij 表示第i句子和第j个句子的相似度
   #[[ x00,x01,x02,x03,x04 ,x05  ]
   # [ x10,x11,x12,x13,x14 ,x15  ]
   # [ x20,x21,x22,x23,x24 ,x25  ]
   # [ x30,x31,x32,x33,x34 ,x35  ]
   # [ x40,x41,x42,x43,x44 ,x45  ]
   # [ x50,x51,x52,x53,x54 ,x55  ]]
    similarities = F.cosine_similarity(y_pred.unsqueeze(1), y_pred.unsqueeze(0), dim=2)
    #这里将similarities 做切片处理，形成下方矩阵
    #[[ x01,x02,x04 ,x05 ]  
    # [x31,x32,x34 ,x35 ]]
    similarities = torch.index_select(similarities,0,row)
    similarities = torch.index_select(similarities,1,col)
    #论文中除以 temperature 超参 
    similarities = similarities / lamda
   #下面这一行计算的是相似矩阵每一行和y_true = [0, 2] 的交叉熵损失
   #[[ x01,x02,x04 ,x05 ]   label = 0 含义：第0个句子应该和第1个句子的相似度最高,  即x01越接近1越好
   # [x31,x32,x34 ,x35 ]]  label = 2 含义：第3个句子应该和第4个句子的相似度最高   即x34越接近1越好
   #这行代码就是simsce的核心部分，和正例句子向量相似度应该越大 
   #越好，和负例句子之间向量的相似度越小越好
    loss = F.cross_entropy(similarities,y_true)
    return torch.mean(loss)
```

