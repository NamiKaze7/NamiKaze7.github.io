# 模型上线

[TOC]

## pytroch serving

https://km.sankuai.com/collabpage/1505419383

### 服务端INFER代码

```python
"""
Author: Zhangyajie06
Date: 2021-10-28
"""
import torch
import art_deployer as deployer
from transformers import BertTokenizer


# 模型的名称必须为 Infer
class Infer:
    # 模型初始化接口, pytorchserving 框架会将参数 configDict 传入
    # 在该方法中使用 configDict 初始化模型
    def initialize(self, configDict):
        # 从modelNameToPath 中取出的是一个Dict元素: key为模型名称，value为模型对应的本地路径.{"model_1":"path_1/1/","model_2":"path_2/1/"}
        self.pathDict = configDict["modelNameToPath"]
        # self.device是设备名称，如果用户开启了GPU的使用： args.use_cuda = True ，那么self.device= torch.device("cuda") ;
        # 如果用户未开启GPU的使用：args.use_cuda = False, 那么self.device = torch.device("cpu")
        self.device = configDict["device"]
        # self.modelType 为模型类型，分为了：torchscript , pytorch, other 三大类。
        self.modelType = configDict["model_type"]
        # self.logger 为logger组件，用户可以用self.logger打出对应的内容信息
        self.logger = configDict["logger"]
        # 从pathDict中根据模型名称对应的路径，加载对应的模型；模型加载时, pathDict 只给出了路径, "standard_bert"就是MLP中心配置的模型名。
        # 注意!!! 使用art_deployer部署时不需要手动添加模型文件的名称，优化模型版本路径下自带一个deploy_config.json文件，art_deployer会自动根据配置文件找到模型并加载。
        self.runner = deployer.load(self.pathDict["standard_bert"], self.logger, self.device)
        # self.tokenizer方法 详见https://huggingface.co/transformers/main_classes/tokenizer.html
        self.tokenizer = BertTokenizer.from_pretrained("bert-base-chinese-vocab.txt")

    # 模型前处理接口, 负责bert模型的tokenize，生成input data
    def preprocess(self, reqDict, modelSpec):
        text = reqDict["input"]
        seq_len = 64
        data = self.tokenizer(text, max_length=seq_len, padding="max_length", truncation=True)
        # print(data)
        preprocessed = {"input": data}
        return preprocessed

    # 模型推理接口, 根据 modelSpec 选择 initialize 初始化的模型进行推理
    def process(self, preprocessed, modelSpec):
        # 使用模型处理推理请求 data 并返回推理结果 pytensor
        data = preprocessed["input"]
        input_ids_tensor = torch.IntTensor(data["input_ids"]).to(self.device)
        attention_mask_tensor = torch.IntTensor(data["attention_mask"]).to(self.device)
        token_type_ids_tensor = torch.IntTensor(data["token_type_ids"]).to(self.device)
        pytensor = self.runner(input_ids_tensor, attention_mask_tensor, token_type_ids_tensor)
        result = {"result": pytensor[1]}
        # 使用模型执行推理 data => pytensor
        return result
```

mlp平台伸缩组参数

```sh
-Dafo.app.container.heartbeat.timeout.seconds=3600 -Dafo.app.env.YARN_DOCKER_CONTAINER_USER_HULK_ENV=staging -Dargs.enable_preprocess=True 


# PMML输出为json
-Drags.use_json_result=true
```

PMML serving部署

```
现有更高版本的jpmml-xgboost-executable jar包转换得到的pmml版本是4.4的，和pmml-serving的1.3.7 evaluator不兼容，会导致afo无法加载pmml模型，有的可以通过将 pmml 文件中版本改为 <PMML xmlns="http://www.dmg.org/PMML-4_3" version="4.3">进行使用（不保证都有效）
```



### Java客户端

https://dev.sankuai.com/code/repo-detail/grocery/grocery-dm-ner/file/list?branch=refs%2Fheads%2Ffeature%2FLQZWM-2021-73170540%2FextractCommentLongText



## hive2mafka

https://km.sankuai.com/page/715437434#id-Step1%EF%BC%9A%E7%94%B3%E8%AF%B7DSN

```sql
##Description##
请描述该ETL任务, 方便理解代码内容, 例如该任务产生什么数据、用于支持什么需求等


##TaskInfo##
tasktype = 'datalink'  ## 不要修改
creator = 'zhanghaozhou@meituan.com'

source = {
    'db': META['hmart_grocerygoodsalgo'],  ##源库对应的dsn名
    'table': 'longcomment_extract_api_unrecall',  ##源表对应的table名
}

target = {
    'db': META['mafka_common_grocerygoodsalgo'],  ## 目标库对应的dsn名
    'table': 'longcomment_sku_match_v1',  ## 目标表的table名
}

## 以下两行 不需要可以删除
## 以下为DataLink同步任务配置，请参考接入文档中的示例模板进行配置
## https://km.sankuai.com/page/28177613

task_conf = {
    "job": {
        "setting": {
            "errorLimit": {
                "percentage": 1
            },
            "flink": {
                "tm": "2048"
            }
        },
        "type":
        "datax",
        "content": [{
            "reader": {
                "name": "hivereader",
                "parameter": {
                    "hiveconf": {
                        "mapred.map.child.java.opts": "-Xmx4096m",
                        "mapreduce.map.memory.mb": "4352"
                    }
                }
            },
            "writer": {
                "name": "mafkawriter",
                "parameter": {
                    "dsnName":
                    "mafka_common_grocerygoodsalgo",  ##-- 【Mafka DSN】（必填）
                    "topicName":
                    "grocery.dm.algo.longcomment_sku_mathch_hive2mafka",  ##-- 【写入的Mafka topicName】（必填）
                    "appkey":
                    "com.sankuai.grocerydm.skucommatch",  ##-- 【Mafka的生产者Appkey】（必填）
                    "serialize":
                    0,  ##-- 【序列化写入Mafka数据的格式】0为按JSON处理，1为按String拼接（必填）
                    "separator":
                    ",",  ##-- 【serialize为1时拼接字符串的分隔符】 默认为","（选填）
                    "debugTopicName":
                    "grocery.dm.assayer.real.hive2mafka",
                    "columns": [  ##-- 【字段及类型】（JSON模式下，字段名必填，类型默认为String）
                        {
                            "name": "dataType"
                        }, {
                            "name": "baseSkuName",
                            "type": "string"
                        }, {
                            "name": "reviewBody",
                            "type": "string"
                        }, {
                            "name": "categoryFirstName",
                            "type": "string"
                        }, {
                            "name": "categorySecondName",
                            "type": "string"
                        }, {
                            "name": "categoryThirdName",
                            "type": "string"
                        }, {
                            "name": "categoryFourthName",
                            "type": "string"
                        }, {
                            "name": "categoryFifthName",
                            "type": "string"
                        }
                    ]
                }
            }
        }]
    }
}


##Extract##
SELECT 'COMMENT_LONG_TEXT' AS dataType,
       base_sku_name AS baseSkuName,
       review_body AS reviewBody,
       category1_name AS categoryFirstName,
       category2_name AS categorySecondName,
       category3_name AS categoryThirdName,
       category4_name AS categoryFourthName,
       category5_name AS categoryFifthName
  FROM mart_grocerygoodsalgo.longcomment_extract_api_unrecall
 WHERE dt='${now.delta(1).datekey}'
   AND recall_label='unrecall'
```

mafka app申请：https://mafka.mws.sankuai.com/process/apply-list?task_id=172819

## 原理

Kafka 是一个分布式流处理平台，可以用来处理和分发大量实时数据。PyTorch 是一个流行的深度学习框架，PyTorch Serving 是一种用于将 PyTorch 模型部署为在线服务的方法。将 Kafka 与 PyTorch Serving 结合，可以在实时数据流上执行实时模型推理。

以下是如何将 Kafka 与 PyTorch Serving 结合进行数据处理的基本步骤：

- 准备 PyTorch 模型
  - 首先，训练一个 PyTorch 模型并将其保存为 .pt 或 .pth 文件。确保模型已经优化并为部署做好了准备。
- 部署 PyTorch Serving
  - 使用 PyTorch Serving（例如，通过使用 TorchServe）将模型部署为在线服务。确保部署后的服务可以处理传入的请求并返回预测结果。
- 安装 Kafka
  - 安装并配置 Kafka 服务器。创建一个用于处理实时数据的 Kafka topic，并确保有一个数据生产者将数据发送到这个 topic。
- 创建 Kafka 消费者
  - 使用 Python Kafka 客户端库（如 `kafka-python`）创建一个 Kafka 消费者。这个消费者将从 Kafka topic 中读取数据，并将数据传输给 PyTorch Serving 服务。
- 将数据发送到 PyTorch Serving
  - 在 Kafka 消费者中，将读取到的数据转换为 PyTorch Serving 可以接受的格式（通常是 JSON 格式）。然后，将数据发送到 PyTorch Serving 进行预测。
- 获取预测结果并处理
  - 从 PyTorch Serving 获取预测结果，并按需进行处理。例如，您可以将结果写回到另一个 Kafka topic，或将其保存到数据库中。

### topic

Kafka是一个分布式流处理系统，它主要用于构建实时数据流管道和实时应用程序。在Kafka中，Topic（主题）是一个关键概念，它表示一类特定类型的消息。Topic可以被视为一个消息的分类或者数据的逻辑通道，它是消息生产者（Producer）和消费者（Consumer）之间进行数据传输的桥梁。

Kafka中的Topic由多个分区（Partition）组成，这些分区允许Kafka在多个服务器上并行处理消息。这种架构提高了Kafka的伸缩性和吞吐量。当生产者产生消息时，它们会将消息发布到一个指定的Topic。消费者则订阅这个Topic，以便从中接收消息。

总之，Kafka的Topic是一种将消息进行分类和分发的机制，它使得生产者和消费者能够高效地传输和处理数据。